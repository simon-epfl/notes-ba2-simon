#import "template.typ": *

#let title = "Com B - Essor de l'I.A."
#let author = ""
#let professor = "Floran Jaton, R√©mi Lebret"
#let creater = "Emmanuel Omont, Simon Lefort"
#let time = "Spring 2024"
#let abstract = "Cours de SHS - Enjeux mondiaux de la communication en lien avec l‚Äôessor de l‚Äôintelligence."
#set quote(block: true)

#show: note_page.with(title, author, professor, creater, time, abstract)

= La loi de Moore

== Les promesses technoscientifiques

- du 16e si√®cle au 18e si√®cle #sym.arrow fonction de sensibilisation
- 19e si√®cle #sym.arrow fonction id√©ologique

=== Ces promesses sont toujours...

- non dystopiques
- imposent des solutions technologiques
- performatives#footnote[\#olympedegouge], le fait de formuler la promesse contribue √† la faire r√©aliser (orientent les moyens allou√©s √† la recherche & innovation)

=== ...et ont pour contraintes...

- la n√©cessit√© de nouveaut√© radicale (la promesse est la solution unique √† un probl√®me urgent)
- cr√©dibilit√© (soutient des sp√©cialistes, quitte √† inventer ces soutiens)

=== Loi de Moore en micro√©lectronique

est un mod√®le pour la fabrication de promesses technoscientifiques \
#sym.arrow.r.curve d√©finit ce qui est pensable pour l'√©volution des micro-processeurs

#box(
  stroke: 1pt + rgb(34, 102, 153), 
  width: 100%, 
  fill: rgb(34, 102, 153, 30), 
  inset: 7pt,
  text(fill : black, [
    #text(fill : rgb(34, 102, 153), 
      [*üìñ Contexte* : apparitions des premiers circuits int√©gr√©s]
    )
    #pad(top: 1pt)[- Fin 1950 $arrow$ Fairchild Seminconductor fabrique des transistors ($"#"$FDS)
    - Pour inciter d'autres acteurs √† faire le pari de l'ouverture de la soci√©t√© civile, Gordon Moore, directeur R&D chez Fairchild, publie un manifeste √©conomique
    - promesse en faveur de l'int√©gration  :
    #quote(attribution: [Moore, 1965])[
      The future of integrated electronics is the future of electronics itself...
    ]]
]))

Seconde formulation de la loi :
#quote(attribution: [Moore, 1975], text([#strike(text("The density has increased at a rate of roughly a factor of two per year")), #text("\nThe density has increased at a rate of roughly a factor of two per two years")]))

Pour les CPU, en 2024, il est probable qu'on se d√©tache de cette loi :
- l'√©nergie est ch√®re
- co√ªts des lieux de fabrication
- la consommation change (+ d'√©conomie d'√©nergie over + de performance)

== depuis 2010 : la course aux GPU

#table(
  columns: (auto, auto),
  inset: 10pt,
  align: horizon,
  table.header(
    [*CPU*], [*GPU*],
  ),
  "Quelques coeurs: entre 2 et 64", "Plusieurs coeurs: entre 2‚Äô000 et 50‚Äô000",
  "Faible latence", "Haut d√©bit de donn√©es",
  "Bon pour le traitement en s√©rie", "Bon pour le traitement en parall√®le",
  "Peut effectuer une poign√©e d‚Äôop√©rations √† la fois", "Peut effectuer des milliers d‚Äôop√©rations √† la fois"
)

Les G.P.U. (graphical processing units) sont tr√®s efficaces pour la multiplication de matrices (donc tr√®s utiles pour l'I.A.).

== Loi de Huang

#quote(attribution: [Jensen Huang (CEO Nvidia, leader des cartes GPU)], "Les performances des GPUs seront plus que doubl√©es chaque 2 ans", )

== Optimisation

*Repr√©sentation des nombres :* r√©duction de la pr√©cision pour acc√©lerer les calculs. (2019, Google, gr√¢ce au brain floating point).

*Nouvelles cartes :* 
- le T.P.U. (Tensor Processing Unit) par Google (2015), con√ßu pour les r√©seaux de neurones (fonctionne avec Tensorflow)
- le L.P.U. (Language Processing Unit) par Groq
- Apple S√©rie M (syst√®me sur une unique puce SoC : CPU, GPU, m√©moire unifi√©)

et beaucoup d'autres entreprises (AWS...)

== Etat du march√©

Nvidia domine, tensions g√©opolitiques li√©es aux semi-conducteurs.

= Les mod√®les de langue

LLM : Large Language Model

== Recette d'un bon LLM

- bcp de param√®tres (x10 chaque ann√©e, nouvelle loi de Moore#footnote[OMG la dinguerie !??!] ?)
- de la puissance de calcul
- bcp (bcp) de donn√©es (seront √©puis√©es en 2026 !)

= Des booms et des hivers de l'IA

Historiquement, les technologies IA ont travers√© des phases de booms et de crises.

== Gen√®se et 1er boom de l'IA (1940‚Äì1965)

Seconde Guerre mondiale #sym.arrow augmentation de la demande en calcul \
- Angleterre #sym.arrow d√©cryptage ("Bomb", "Coloss") \
- U.S.A. #sym.arrow calcul balistique (ENIAC)

1942 : Moore School of Electrical Engineering, pour acc√©l√©rer la production de tables de tir.

1943 : \$400k allou√©s √† la constructeur de l'ENIAC (le dispositif prend le nom de "computer") \
#sym.arrow.r.curve tr√®s innovant mais probl√®me d'architecture

1944 : nouveau projet d√©riv√© de l'ENIAC, le *EDVAC* (notament gr√¢ce √† John von Neumann).

Formalisation de l'*architecture Von-Neumann* (encore tr√®s utilis√©e aujourd'hui) :

#image("neumann.png")

Juin 1945 : Neumann publie un rapport sur EDVAC (utilise des analogies avec le cerveau pour la premi√®re fois)

1949 : 1er ordinateur BINAC (sert de r√©f√©rence √† UNIVAC, 1951), les ordinateurs/calculateurs sortent progressivement de la recherche militaire #sym.arrow *industrie et administration*.


1956 : 1√®re appartition du terme I.A. (John McCarthy)

1960 : premier boom de l'IA dite "symbolique" \
#sym.arrow.r.curve recherche logicielle visant √† d√©crire les r√®gles de pens√©e et les exprimer sous forme de code informatique (par ex. chatbot ELIZA).

Un groupe ferm√© de chercheurs s'arroge le monopole de la d√©finition des enjeux de l'IA. \
#sym.arrow.r.curve capture l'essentiel des financements (75% de US Air Force) \
#sym.arrow.r.curve conserve l'acc√®s aux grands syst√®mes informatiques

... cela conduit au premier hiver.

== Premier hiver de l'IA (1965-1975)

#sym.arrow promesses des promoteurs de l'IA symbolique n'ont pas √©t√© tenues \
#sym.arrow √† partir de 1970, baisse des financements (notamment militaires) \
#sym.arrow accus√©s de se concentrer sur des "mondes jouets"

== Syst√®mes experts experts et 2√®me boom (1975-1985)

Renouveau de l'IA symbolique (ordi + puissant et d√©composition des processus de raisonnement en briques √©l√©mentairs)

*Syst√®me expert :*

#image("expert.png")

1970 : syst√®me MYCIN \
#sym.arrow s√©rie de questions au m√©decin \
#sym.arrow env. 600 r√®gles \
#sym.arrow produit une liste de bact√©ries candidates

1980 : XCON pour aider √† configurer les ordinateurs

1984 : DELTA pour identifier les pannes sur les locomotives

*Limites des syst√®mes expert :*

#quote(attribution: [Edward Feigenbaum, 1983])[
  Dans les d√©cennies √† venir, nous devrons disposer de moyens plus automatiques pour remplacer ce qui est actuellement une proc√©dure tr√®s fastidieuse, longue et co√ªteuse.
]

== Deuxi√®me hiver et travail de l'ombre

Des promesses non tenues (encore...) \
#sym.arrow probl√®me de hardware \
#sym.arrow probl√®me de maintenance des logiciels \
#sym.arrow la plupart des startups ont fait faillite

1990 : IA symbolique est si affaiblie que le terme dispara√Æt quasiment du vocabulaire de recherche.

=== Parallel Distributed Processing

1986 : √† l'√©cart de l'IA symbolique, un groupe de chercheurs travaille sur les *r√©seaux de neurones* (notamment reconnaissance des codes postaux)

notion de r√©tropropagation du gradient (ajuster les param√®tres du mod√®le en fonction des erreurs qu'il commet, en fait le gradient c'est la d√©riv√©e, on l'utilise pour savoir dans quelle direction aller pour minimiser l'erreur).

LeNet-1, reconnaissance de chiffres - 1989 \
LeNet-5, reconnaissance de caract√®res (reconnaissances de ZIP codes par ex.) - 1998

== R√©seau de neurones profonds et 3√®me boom (2005-2024)

=== Av√®nement du Deep Learning :
- puissance de calcul augmente (performances des cpu #sym.arrow.tr + GPU)
- r√©seaux de neurones + profonds

=== Num√©risation et essort d'Internet

- quantit√© de donn√©es #sym.arrow.tr
- mise en place de plateformes de crowdsourcing (pour labelliser des donn√©es)

Exemple : ImageNet (1k cat√©gories d'objets, 1.2M d'images)

2012 : AlexNet, reconnaissance d'objets, meilleurs performances sur ImageNet (25% #sym.arrow 16%) gr√¢ce aux *filtres de convolution*.

2015 : ResNet

#sym.arrow application d‚Äôune m√™me transformation lin√©aire sur diff√©rentes zones de l‚Äôimage \
#sym.arrow on part de petites matrices (3x3) en demandant au mod√®le de g√©n√©rer de grandes matrices (5x5) *, chaque groupe est une couche de convolution* (pour √©viter de se concentrer sur les d√©tails).

Il y a plusieurs *cartes d'activation* (filtre = feature, appris √† l'entra√Ænement) √† la sortie de chaque couche.

R√©duction des cartes d‚Äôactivation par *op√©ration de pooling*.

Chaque convolution est suivie d'une *fonction d'activation* non lin√©aire.

La derni√®re couche est la couche *de classification*, qui d√©termine la sortie avec poids (appris pendant l'entra√Ænement).

= Des booms et des hivers de l'IA (II)

== Gen√®se: memorandum de Weaver et d√©mos. publiques (1950-1965)

1949 : Weaver sugg√®re une meilleure approche (statistique et probabiliste) que celle de la traduction lin√©aire

1954 : premi√®re d√©monstration publique √† New York, traduction de russe √† anglais en public.

== Crise: le rapport ALPAC et ses cons√©quences (1965-1990)

1966 : le rapport ALPAC (Automatic Language Processing Advisory Committee)

#quote(attribution: [National Research Council, 1966])[Il n‚Äôy a aucune urgence dans le domaine de la traduction automatique. Le
probl√®me n‚Äôest pas de satisfaire un besoin inexistant √† travers des
syst√®mes de traduction automatiques inexistants]

peu de b√©n√©fices √† court-terme #sym.arrow *chute drastique des financements.*

== Renouveau et tradition statistique (1990-2015)

1990 : apparition de corpus parall√®les#footnote[Merci au parlement canadien d'avoir traduit gratuitement des textes anglais-fran√ßais. Sinon, on utilise aussi la Bible, vu que c'est un texte traduit dans quasiment 100% des langues] (utiles pour la traduction)

1992 : rapport JTEC (Japan Technology Evaluation Center) et incitations politiques (convaincre les gouvernements d'utiliser les nouvelles technologies)

mais aussi puissance de calcul et stockage #sym.arrow.tr \
et nouvelle culture statistique, probabiliste

=== Focus sur mod√®les de traduction bas√©s sur les groupes de mots

1993 : IBM introduit plusieurs mod√®les statistiques pour la traduction (corpus parall√®les issus du parlement canadien)

2006 : Google Translate, bas√©e sur cette m√©thode

==== Comment √ßa marche ?

- segmentation des phrases en groupe de mots (tokens)
- recherche de correspondances les + probables
- assemblage des correspondances

==== Limitations de l‚Äôapproche statistique

- traduction fausse si syntaxe non courante
- utilisation de l'anglais comme "langue pivot" (FR #sym.arrow EN #sym.arrow IT)#footnote[le truc dr√¥le en plus, c'est si vous le mettez en PLS il sortira une phrase de la Bible]

== Traduction automatique par r√©seau de neurones (2014-2024)

=== Boom des ConvNets

Entre 2012 et 2015 : boom des ConvNets pour traitement des images

==== Comment √ßa marche ?

Comment apprendre le langage naturel avec des r√©seaux de neurones?

- les machines comprennent le langage binaire
- les r√©seaux de neurones doivent recevoir en entr√©e des donn√©es continues
- le texte est repr√©sent√© par des symboles discrets (lettres, chiffres, caract√®res sp√©ciaux, etc.)

*Probl√®me :* avec ASCII, l'encodage binaire, un mot n'est pas d√©fini par ses lettres, impossibilit√© d'apprendre le sens d'un mot avec binaire. (ex chouette #sym.eq.not brouette).

*Solution :* encodage one-hot : gr√¢ce au word embeddings un graph bas√© sur le sens tous les mots va se former.

Pour cela : on prend la probabilit√© de coocurrence P(c/w), puis on r√©duit les dimensions trouv√©es pour √™tre + ou - pr√©cis dans la compr√©hension (gr√¢ce √† la SVD).

#sym.arrow une m√©thode efficace mais peu efficiente (√ßa a pris 4 mois pour s'entra√Æner sur Wikipedia).

==== word2vec (2013)

- mod√®le lin√©aire
- tricks pour am√©liorer l'apprentissage des mots rares
- code open source en C
- apprentissages de word embeddings en quelques minutes

Exemple : d√©terminer si un avis est positif ou n√©gatif.

=== R√©seaux r√©currents

- adapt√© au langage, qui est s√©quentiel

2014 : premiers r√©seaux de neurones r√©currents pour la traduction
automatique

=== Long-Short Term Memory (LSTM) Network (traduction)

Probl√®me de l'√©poque : apprentissage des r√©seaux r√©currents difficile pour les longues
s√©quences.

Une id√©e des ann√©es 90 refait surface: LSTM networks.

2016: Google Translate opte pour un mod√®le de traduction neuronal bas√© sur les LSTM

==== Limites des LSTM-RNN

- probl√®me d'optimisation
- mod√®les s√©quentiels difficilement parall√©lisables
- architecture peu efficace sur GPU
- temps d'apprentissage + long

=== Transformer: Attention Is All You Need

- 2017: nouvelle architecture bas√©e uniquement sur le m√©canisme d‚Äôattention
- efficace sur GPU (entra√Ænement + rapide)
- les RRN n'ont pas dit leur dernier mot (transformer + rapide en entra√Ænement mais gourmands en m√©moire)
- Mars 2024: Google DeepMind pr√©sente deux nouveaux mod√®les de langue bas√©s sur des RNNs

*M√©canisme d'attention :* fait attention √† n'importance des mots en fonction du contexte des mots en l'entourant.

= Supervision et apprentissage automatique

== Apprentissage supervis√©

Une fois qu'un jeu de donn√©es est disponible, les chercheurs la divise en 2 sous-ensembles :
- jeu d'entra√Ænement
- jeu d'√©valuation

=== Des avanc√©es et des limites

- biais de s√©lection des donn√©es
- biais des annotations des donn√©es
- co√ªt de l'annotation des donn√©es (contra√Æntes de temps : ex. annotation de contrats juridiques 1/4 de temps juste pour les annoter).

== Apprentissage auto-supervis√©

Yann Le Cun ‚ÄúCake Analogy‚Äù : la grosse partie du g√¢teau est faite avec un apprentissage non supervis√©, et le gla√ßage, la cerise, est faite avec un peu d'apprentissage supervis√©.

=== Apprentissage des donn√©es d'entr√©e

- apprendre √† pr√©dire le mot d‚Äôapr√®s, ou les mots cach√©s
- GPT (Generative Pre-Trained Transformer) #sym.arrow pr√©dire le mot suivant
- apprendre des images avec des t√¢ches "pr√©textes" pr√©d√©finies (rotation, mettre en couleur)

=== Apprentissage discriminatif

utilise un signal discriminant entre les images, permet de classifier les images

=== Apprentissage contrastif

Apprendre √† d√©terminer si une paire d'image est positive ou pas (donc si les deux images sont de la m√™me cat√©gorie) \
On donne des paires d'images positives, c'est-√†-dire d'un m√™me set, et des paires d'images n√©gatives. Ensuite pour continuer √† l'entra√Æner on lui donne des paires avec une image A et cette image A retourn√©e, et on v√©rifie si le r√©sultat est bien "positif" (c'est les t√¢ches pr√©textes).

Exemple : *mod√®le CLIP*, auto-supervision avec 400M de paires (image, description) collect√©es sur le web. Performances √©quivalentes √† ImageNet. Meilleures g√©n√©ralisation des donn√©es inconnues.

== Apprentissage par instruction (objectif: alignement)

Les mod√®les bas√©s sur l'apprentissage auto sont limit√©s, ils ne savent que pr√©dire le mot suivant.

*Solution : s'aligner sur les attentes des utilisateurs* :

#box(
  stroke: 1pt + rgb(34, 102, 153), 
  width: 100%, 
  fill: rgb(34, 102, 153, 30), 
  inset: 7pt,
  text(fill : black, [
    #pad(top: 1pt)[
    Instruction: Traduit la phrase suivante en Fran√ßais. \
Observation: The cat sat on the mat. \
Label: [...pr√©dit par le mod√®le...]] 
]))

Cependant, les donn√©es annot√©es restent co√ªteuses et limit√©es. Ainsi, on peut utiliser d'autres mod√®les pr√©-entra√Æn√©s pour g√©n√©rer des instructions.

Exemple : *Alpaca dataset,* 52 000 instructions g√©n√©r√©es avec GPT-3.5 √† partir de 175 instructions.

*Solution am√©lior√©e : s'aligner sur les domaines sp√©cialis√©s *

Exemple : Google Med-PaLM (mod√®le sous license propri√©taire) adapt√© depuis PaLM.

*A int√©grer √† la solution : s'aligner sur les valeurs humaines*

- le web contient des donn√©es toxiques ou mensong√®res
- human in the loop : humains qui v√©rifient les donn√©es

=== Les limites de l'alignement

- Gemini image √©tait woke
- il est actuellement dans les faits impossible de sortir de la supervision
- les mod√®les d'IA qu'on utilise ont √©t√© supervis√©s avec des biais arbitraires
- la seule piste qu'on a est de rendre explicite les valeurs qui sous-tendent le process d'annotation et d'alignement#footnote[basiquement on va juste dire "oui j'assume que notre mod√®le est biais√© et pense que la terre est plate, d√©sol√© on y peut rien nos donn√©es venaient des platistes"]

= IA Generative & sph√®re informelle

== Quelques exemples

- Pape Fran√ßois en doudoune, g√©n√©r√© par un employ√© du BTB √† Chicago, relay√©e sur twitter
- Donald Trump arr√™t√© par la police de NY, dans un contexte tendu
- Emmanuel Macron au milieu de la r√©forme des retraites

#sym.arrow √Ä Davos, tous les regards sont sur les deepfakes.

== Generateur d'images par IA

=== IA generative VS IA discriminative

- Discriminatif : focus sur les caract√©ristiques qui distinguent les diff√©rentes cat√©gories (objectif : reconna√Ætre une image)
- G√©n√©ratif : mod√©lisation de la distribution des donn√©es

=== Type de mod√®les pour la distribution d'images.

*Auto-Encodeur : *\
Apprendre une distribution approxim√©e d'un ensemble de points ayant une distribution inconnue.

Cool : compresser des images dans un espace r√©duit, apprentissage auto-supervis√©.\
Pas cool : Qualit√© mauvaise, manque de contr√¥le sur la g√©n√©ration (on ne peut pas dire o√π sont les yeux d'une image par ex).

*Auto-Encodeur variationnel * \

Apprentissage d'une repr√©sentation probabiliste et continue de l'entr√©e \
Le d√©codeur g√©n√®re une image √† partir d'une variable latente

Cool : Repr√©sentation plus structur√©e, images + diversifi√©es, bcp plus de contr√¥le \
Pas cool : On g√©n√®re l'image en une √©tape (VAE)

* Mod√®le de diffusion * \

On g√©n√®re l'image en plusieurs √©tapes. \
#sym.arrow.curve On apprend √† pr√©dire le bruit d'une image avec un encodeur, qui "bruite" l'image au fur et √† mesure (√† la fin on a un espace r√©duit ducoup), apr√®s, on apprend √† reconstruire l'image avec un d√©codeur. \

forward diffusion : obtenir le bruit √† partir de l'image \
reverse diffusion : l'inverse.

On a une cha√Æne de markov, avec des √©tapes petit √† petit.

=== Entrainement pour mod√®le de diffusion

1. Prend une image
2. G√©n√©rer du niveau de bruit
3. On choisit un niveau de bruit
4. Ajouter le bruit √† l'image
5. On lui demande de pr√©dire le bruit qui a √©t√© ajout√© √† l'image. \
#sym.arrow.curve Une fois entra√Æn√©, on peut pr√©dire l'image suivante en retirant le bruit que le mod√®le pense qui a √©t√© ajout√©.

=== Conditionner la g√©n√©ration d'image

On va ajouter du texte √† notre jeu d'entrainement.\
#sym.arrow.curve On utilise des mod√®les qui ont d√©j√† labelis√© les images (mod√®le CLIP) #sym.arrow Chaque mot est repr√©sent√© par du text embedding. \
#sym.arrow.curve Le pr√©dicteur de bruit avec m√©canisme d'intention va g√©n√©rer une image al√©atoire conditionn√©e avec le texte entr√© \
#sym.arrow.curve On peut aussi conditionner en disant "eh √† cet endroit tu me met un train"#footnote[Si vous voulez voir √† quoi √ßa ressemble concr√®tement, vous pouvez aller voir Nvidia Canvas]

=== Principaux mod√®les de diffusion text-to-image

1. Midjourney (v1 #sym.arrow v6)
2. Dalle-E (2 #sym.arrow 3)
3. Imagen ( #sym.arrow 2)
4. Stable diffusion (v1 #sym.arrow v3)

=== Controverse de g√©n√©ration d'image

Des scientifiques ont publi√© un papier avec des images g√©n√©r√©es par Midjourney, ils se sont fait taper dessus

== Fausses images et sph√®re informationnelle

Jusqu'√† pr√©sent, il n'y a pas eu de r√©percussions g√©opolitiques majeures#footnote[o√π on ne le sait pas encore MDR], mais ces images g√©n√®res des inqui√©tudes. \
#sym.arrow.curve Elles mettent √† mal la recherche d'image invers√©e et l'analyse des retouches (les 2 check des s√©curit√©s)

Ces fausses images continuent de mettre de l'huile sur le feu dans un monde avec d√©j√† beaucoup de guerres.

Point de d√©part de la guerre informationnelle : *Guerre du Golfe*, quand le gouvernement du Koweit engage de nombreuses ressources pour mobiliser l'opinion publique contre Saddam Hussein (ex. t√©moignage vid√©o de Nahira, vu par 60,000,000 d'am√©ricains *qui √©tait un faux !*). \
#sym.arrow.r.curve aide bcp le Koweit

Tous les pays commencent √† prendre au s√©rieux cette domination informationnelle.

*D√©but ann√©es 1990 :* Iran, mise en place de l'IRIB. un peu comme CNN, 1Md de budget, 15k employ√©s, mais l'information est orient√©e pour aller dans le sens de l'islam

*1996* : fondation d'Al Jazeera par le Qatar, sur le mod√®le aussi de CNN, constitue un canal pour les d√©clarations du Hamas, du Hezbollah (sud Liban), et d'Al-Qa√Øda.

*1998* : op√©ration bouclier dor√© (contr√¥le de l'acc√®s des internautes chinois aux contenus √©trangers). En 2022 : 73% du trafic internet chinois provient de l'int√©rieur de ses fronti√®res !

*1994* : syst√®me SORM (Syst√®me pour Activit√© d'Enqu√™te Op√©ratoire) permet d'intercepter l'ensemble des communications sur l'ensemble des territoires russophones.

*2005* : lancement du m√©dia Russia Today (propagande)

*2016* : grande campagne de d√©stabilisation informationnelle, lors de la campagne √©lectoral entre Trump et Hillary Clinton (piratage des bo√Ætes mails d'Hillary Clinton)

Situation pr√©occupante de chaos informationnelle.

2024 : Annonce faite par le Hamas (bombardement de l'hopital Al-Ahli) #sym.arrow relay√©e par la presse internationale... alors que c'√©tait faux. Cela a conduit √† une tr√®s forte augmentation des manifestations pro-palestine.

== Mais il reste de l'espoir !

Le propri√©taire du journal The Guardian est dirig√© par une fondation, √† but non lucratif. Il assure une ind√©pendance d'√©criture, car les journalistes n'ont pas besoin de "rechercher le scoop".

De m√™me Le Temps est sorti d'une entreprise pour se faire racheter par une fondation, Aventinus, cr√©√©e par des personnes riches pour soutenir la presse romande. *le comit√© de r√©daction est s√©par√© du conseil de fondation !* pas de conflit d'int√©r√™t.
